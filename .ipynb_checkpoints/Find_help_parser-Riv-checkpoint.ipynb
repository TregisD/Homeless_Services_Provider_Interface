{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb9ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os \n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "701a1ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading html files from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a6e0ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Riv_Mental_Health_HTML_Files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRiv_Mental_Health_HTML_Files\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Loop through each file in the folder\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(folder_path):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     11\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Riv_Mental_Health_HTML_Files'"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "#'Riv_Shelter_HTML_Files'is the name for the shelter HTML Files\n",
    "#'Riv_Food_Pantry_HTML_Files'is the name for the food pantry HTML Files\n",
    "#'Riv_Mental_Health_HTML_Files'is the name for the shelter HTML Files\n",
    "folder_path = 'Riv_Mental_Health_HTML_Files'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.html'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "\n",
    "        # Find the <ul> element with class \"best-programs\"\n",
    "        ul_element = soup.find('ul', {'class': 'best-programs'})\n",
    "\n",
    "        # Find all <li> elements within the <ul>\n",
    "        li_elements = ul_element.find_all('li', {'class': 'search-result card card-v3 program-info'})\n",
    "\n",
    "        # Loop through the found <li> elements and extract labels and values\n",
    "        for li_element in li_elements:\n",
    "            # Use BeautifulSoup to parse the HTML of each li_element\n",
    "            li_soup = BeautifulSoup(str(li_element), 'html.parser')\n",
    "\n",
    "            # Find the program heading \n",
    "            program_heading_element = li_soup.find('div', {'class': 'card-heading'})\n",
    "            Service_name = program_heading_element.find('a', {'class': 'activity-log ph-flyout-click cwdc-flyout-click click-cookie'}).text\n",
    "            print('Service name: ', Service_name)\n",
    "    \n",
    "            # Extract program URL\n",
    "            program_url_element = program_heading_element.find('a', {'class': 'activity-log ph-flyout-click cwdc-flyout-click click-cookie'})\n",
    "            program_url = program_url_element['href'] if program_url_element else None\n",
    "            Service_url = \"https://www.auntbertha.com/\"+program_url\n",
    "            print('URL: ', Service_url)\n",
    "            \n",
    "            is_reviewed = False\n",
    "\n",
    "            # Reviewed on\n",
    "            Reviewd_on_element = program_heading_element.find('div', {'class': 'last-reviewed'})\n",
    "            if Reviewd_on_element:  # Check if the element exists\n",
    "                Reviewd_on_text = Reviewd_on_element.get_text(strip=True)  # Clean up the text\n",
    "                match = re.search(r'\\d{2}/\\d{2}/\\d{4}', Reviewd_on_text)\n",
    "                Reviewd_on = match.group() if match else None\n",
    "                if Reviewd_on:  # If we found a review date\n",
    "                    is_reviewed = True  # Set the flag to True\n",
    "                print(\"Reviewed on:\", Reviewd_on)\n",
    "            else:\n",
    "                Reviewd_on = None  # Set to None if not found\n",
    "                print(\"No review date found.\")\n",
    "    \n",
    "            # Access class=\"program-tags\"\n",
    "            program_tags = li_soup.find('div', {'class': 'program-tags'})\n",
    "\n",
    "            # Main Services\n",
    "            main_s = []\n",
    "            main_service_list = program_tags.find('ul', {'class': 'list-inline'})\n",
    "            main_service_items = main_service_list.find_all('li')\n",
    "\n",
    "            for item in main_service_items:\n",
    "                main_service = item.find('a', {'class': 'loading-on-click activity-log'}).text.strip()\n",
    "                main_s.append(main_service)\n",
    "\n",
    "            print(\"Main Services:\", main_s)\n",
    "\n",
    "            # Try to access the \"Other Services\" section\n",
    "            other_service_list = program_tags.find('div', {'class': 'secondary-tags'})\n",
    "            if other_service_list:\n",
    "                other_s = []\n",
    "                other_service_items = other_service_list.find('ul', {'class': 'list-inline'}).find_all('li')\n",
    "\n",
    "                for item in other_service_items:\n",
    "                    other_service = item.find('a', {'class': 'loading-on-click activity-log'}).text.strip()\n",
    "                    other_s.append(other_service)\n",
    "            else:\n",
    "                other_s = None\n",
    "\n",
    "            print(\"Other Services:\", other_s)\n",
    "\n",
    "            # Access the \"Serving\" section\n",
    "            serving_section = program_tags.find('div', {'class': 'attribute-tags'})\n",
    "\n",
    "            # Initialize a list to store the serving information\n",
    "            serving_ = []\n",
    "    \n",
    "            # Find all the <li> elements within the serving section\n",
    "            serving_items = serving_section.find_all('li')\n",
    "\n",
    "            # Iterate through the serving items and extract the text from the <a> elements\n",
    "            for item in serving_items:\n",
    "                link = item.find('a', {'class': 'loading-on-click activity-log'})\n",
    "    \n",
    "                # Check if the link was found\n",
    "                if link:  # Only proceed if the link exists\n",
    "                    serving_text = link.text.strip()\n",
    "                    serving_.append(serving_text)\n",
    "\n",
    "            print(\"Serving:\", serving_)\n",
    "\n",
    "    \n",
    "            # accessing next-steps-module, extract phone number, location, hours \n",
    "    \n",
    "            next_steps_module = li_soup.find('div', {'class': 'next-steps-module'})\n",
    "\n",
    "            # Extract phone number\n",
    "            phone_number_elements = next_steps_module.find_all('span', {'class': 'result-next-step-item'})\n",
    "\n",
    "            # Also find all 'a' elements with href attributes containing 'tel:'\n",
    "            tel_link_elements = next_steps_module.find_all('a', href=True)\n",
    "\n",
    "            phone_number = None  # Initialize phone number as None\n",
    "\n",
    "            # Extract phone numbers from text-based spans\n",
    "            for element in phone_number_elements:\n",
    "                phone_number_text = element.text.strip() if element else None\n",
    "    \n",
    "                # Regex to extract digits (handling separators like spaces or hyphens)\n",
    "                phone_number_matches = re.findall(r'[\\d-]+', phone_number_text)\n",
    "    \n",
    "                # Join the digits into a single phone number string if matches are found\n",
    "                if phone_number_matches:\n",
    "                    phone_number = ''.join(phone_number_matches)\n",
    "                    break  # Stop once we've found a phone number\n",
    "\n",
    "            # If no phone number found from text, check the 'tel:' href links\n",
    "            if not phone_number:\n",
    "                for element in tel_link_elements:\n",
    "                    href_value = element['href']\n",
    "        \n",
    "                    # Check if the href contains 'tel:' and extract digits\n",
    "                    if 'tel:' in href_value:\n",
    "                        phone_number_matches = re.findall(r'[\\d-]+', href_value)\n",
    "                        if phone_number_matches:\n",
    "                            phone_number = ''.join(phone_number_matches)\n",
    "                            break  # Stop once we've found a phone number\n",
    "\n",
    "            # Print the extracted phone number\n",
    "            if phone_number:\n",
    "                print(\"Phone Number:\", phone_number)\n",
    "            else:\n",
    "                print(\"No phone number found.\")\n",
    "\n",
    "            # Extract location address\n",
    "            location_address_element = next_steps_module.find('a', {'class': 'activity-log ph-flyout-click cwdc-flyout-click map-link with-address'})\n",
    "            location_address = location_address_element.text.strip() if location_address_element else None\n",
    "            location_address = re.sub(r'\\s+', ' ', location_address) if location_address_element else None\n",
    "            print(\"Location Address:\", location_address)\n",
    "\n",
    "            # Extract URL\n",
    "            location_url_map = location_address_element['href'] if location_address_element else None\n",
    "            print(\"Location url map:\", location_url_map)\n",
    "            \n",
    "            hours_info = {\n",
    "                '24_hour': False,\n",
    "                'Monday': None,\n",
    "                'Tuesday': None,\n",
    "                'Wednesday': None,\n",
    "                'Thursday': None,\n",
    "                'Friday': None,\n",
    "                'Saturday': None,\n",
    "                'Sunday': None\n",
    "            }\n",
    "            \n",
    "            # Extract hours based on structure\n",
    "            hours_element = next_steps_module.find('div', {'class': 'office-hours-schedule see-hours-dropdown'})\n",
    "            if hours_element:\n",
    "                # Find all the <span> elements within the hours_element\n",
    "                day_spans = hours_element.find_all('span')\n",
    "\n",
    "                # Ensure that we have even pairs of day and corresponding hours\n",
    "                if len(day_spans) % 2 == 0:\n",
    "                    # Iterate through the spans in pairs (day and its corresponding hours)\n",
    "                    for i in range(0, len(day_spans), 2):\n",
    "                        day_span = day_spans[i]          # This is the day (e.g., \"Monday:\")\n",
    "                        hours_span = day_spans[i + 1]    # This is the corresponding hours (e.g., \"Closed\" or time)\n",
    "\n",
    "                        # Extract the day name and remove the colon\n",
    "                        day = day_span.text.strip()[:-1]  # Remove the ':' at the end\n",
    "                        full_day = day.strip()            # Ensure there's no extra space\n",
    "\n",
    "                        # Check if the day is valid and exists in hours_info\n",
    "                        if full_day in hours_info:\n",
    "                            # Check if the corresponding hours span indicates \"Closed\"\n",
    "                            if 'Closed' in hours_span.text:\n",
    "                                hours_info[full_day] = 'Closed'\n",
    "                            else:\n",
    "                                # Extract hours for non-closed days\n",
    "                                hours_info[full_day] = hours_span.text.strip()\n",
    "                else:\n",
    "                    print(\"Unexpected hours format. Ensure that each day has corresponding hours.\")\n",
    "\n",
    "            # Handle 24-hour information if it exists\n",
    "            else:\n",
    "                hours_element = next_steps_module.find('span', {'class': 'result-geo-hours'})\n",
    "                if hours_element:\n",
    "                    # Get 24-hour text strip\n",
    "                    hours_text = hours_element.get_text(strip=True)\n",
    "\n",
    "                    # Assuming the presence of this text means 24-hour operation\n",
    "                    if '24' in hours_text or '24-hour' in hours_text.lower():\n",
    "                        hours_info['24_hour'] = True\n",
    "                    else:\n",
    "                        print(\"Unable to determine if the office is 24 hours.\")\n",
    "                else:\n",
    "                    print(\"Hours information not available.\")\n",
    "\n",
    "            # Output the result\n",
    "            print(hours_info)\n",
    "    \n",
    "    \n",
    "            # Loop through the found <li> elements and extract labels and values\n",
    "            Extra_element = li_element.find('div', {'class': 'panel-wrapper more-info-panel'})\n",
    "            elig = []\n",
    "            eligibility_rules_element = Extra_element.find('div', {'class': 'eligibility-rules'})\n",
    "    \n",
    "            # Check if eligibility_rules_element is found\n",
    "            if eligibility_rules_element:\n",
    "                # Check if eligibility_rules_element contains a list (ul)\n",
    "                ul_element = eligibility_rules_element.find('ul')\n",
    "        \n",
    "                if ul_element:\n",
    "                    # If it contains a list, extract list items and store them in a flat list\n",
    "                    eligibility_list = [li.text.strip() for li in ul_element.find_all('li')]\n",
    "                    elig.extend(eligibility_list)  # Use extend to add elements to the list directly\n",
    "                    #print(\"Eligibility:\", eligibility_list)\n",
    "                else:\n",
    "                    # If it doesn't contain a list, store the text as is\n",
    "                    eligibility_text = eligibility_rules_element.text.strip()\n",
    "                    #print(\"Eligibility:\", [eligibility_text])  # Wrap in a list to maintain consistency\n",
    "                    elig.append(eligibility_text)\n",
    "\n",
    "            else:\n",
    "                # Skip if 'eligibility-rules' class is not found\n",
    "                pass\n",
    "    \n",
    "        \n",
    "            print(\"Eligibility:\", elig)\n",
    "    \n",
    "            # Extract Availability\n",
    "            availability_element = Extra_element.find('strong', {'data-translate': 'Availability'})\n",
    "            \n",
    "            if availability_element:\n",
    "                availability = availability_element.find_next('div', {'class': 'col-md-10'}).text.strip()\n",
    "            else:\n",
    "                availability = \"Not specified\"\n",
    "\n",
    "            # Extract Description\n",
    "            description_element = Extra_element.find('strong', {'data-translate': 'Description'})\n",
    "            description = description_element.find_next('div', {'class': 'col-md-10'}).text.strip()\n",
    "\n",
    "            # Extract Languages\n",
    "            languages_element = Extra_element.find('strong', {'data-translate': 'Languages'})\n",
    "            languages = languages_element.find_next('div', {'class': 'col-md-10'}).text.strip()\n",
    "            languages_data = [lang.strip() for lang in languages.split(',')]\n",
    "            \n",
    "            # Extract Cost\n",
    "            cost_element = Extra_element.find('strong', string='Cost:')\n",
    "            \n",
    "            if cost_element:  # Check if cost_element exists\n",
    "                cost = cost_element.find_next('div', {'class': 'col-md-10'}).text.strip()\n",
    "            else:\n",
    "                cost = \"Not specified\"  # Default value if cost_element is not found\n",
    "            \n",
    "            # Extract Website URLs if they exist, or set them to None\n",
    "            website_element = Extra_element.find('div', {'data-translation': 'Website'})\n",
    "            website_url = website_element.find_next('a', {'class': 'activity-log descriptionProgramWebsite'})['href'] if website_element else None\n",
    "\n",
    "            # Extract Facebook and Twitter URLs if they exist, or set them to None\n",
    "            facebook_element = Extra_element.find('strong', {'data-translate': 'Facebook'})\n",
    "            facebook_url = facebook_element.find_next('a', {'class': 'activity-log descriptionProgramFacebook'})['href'] if facebook_element else None\n",
    "\n",
    "            twitter_element = Extra_element.find('strong', {'data-translate': 'Twitter'})\n",
    "            twitter_url = twitter_element.find_next('a', {'class': 'activity-log descriptionProgramTwitter'})['href'] if twitter_element else None\n",
    "\n",
    "            # Extract Coverage Area\n",
    "            coverage_element = Extra_element.find('strong', {'data-translate': 'Coverage Area'})\n",
    "            coverage = coverage_element.find_next('div', {'class': 'col-md-10'}).text.strip()\n",
    "            \n",
    "            # Initialize latitude and longitude as None by default\n",
    "            latitude = None\n",
    "            longitude = None\n",
    "            zipcode = None\n",
    "\n",
    "            # Find the element with class \"office-hour-address\"\n",
    "            location_element = Extra_element.find('div', {'class': 'office-hours-address _js_address address notranslate'})\n",
    "\n",
    "            # Check if the element exists and has the required attributes\n",
    "            if location_element:\n",
    "                latitude = location_element['data-latitude'] if location_element.has_attr('data-latitude') else None\n",
    "                longitude = location_element['data-longitude'] if location_element.has_attr('data-longitude') else None\n",
    "                print(\"Latitude:\", latitude)\n",
    "                print(\"Longitude:\", longitude)\n",
    "                \n",
    "                # Extract all text within location_element\n",
    "                address_text = location_element.get_text(separator=\" \").strip()  # Get all text as a single string\n",
    "\n",
    "                # Use regular expression to search for the ZIP code pattern anywhere in the text\n",
    "                zip_matches = re.findall(r'(?<!\\d)(\\b\\d{5}\\b)(?!\\d)', address_text)\n",
    "                zipcode = int(zip_matches[-1]) if zip_matches else None\n",
    "                print(\"ZIP Code:\", zipcode)\n",
    "\n",
    "            # Print or use the extracted values as needed\n",
    "            print(\"Availability:\", availability)\n",
    "            print(\"Description:\", description)\n",
    "            print(\"Languages:\", languages_data)\n",
    "            print(\"Cost:\", cost)\n",
    "            print(\"Facebook URL:\", facebook_url)\n",
    "            print(\"Twitter URL:\", twitter_url)\n",
    "            print(\"Coverage Area:\", coverage)\n",
    "\n",
    "            data.append([\n",
    "            Service_name,\n",
    "            Service_url,\n",
    "            main_s,\n",
    "            other_s,\n",
    "            serving_,\n",
    "            phone_number,\n",
    "            website_url,\n",
    "            location_address,\n",
    "            location_url_map,\n",
    "            elig,\n",
    "            availability,\n",
    "            description,\n",
    "            languages_data,\n",
    "            cost,\n",
    "            is_reviewed,\n",
    "            facebook_url,\n",
    "            twitter_url,\n",
    "            coverage,\n",
    "            latitude,\n",
    "            longitude,\n",
    "            zipcode,\n",
    "            hours_info['24_hour'],\n",
    "            hours_info['Monday'],      \n",
    "            hours_info['Tuesday'],     \n",
    "            hours_info['Wednesday'],   \n",
    "            hours_info['Thursday'],   \n",
    "            hours_info['Friday'],     \n",
    "            hours_info['Saturday'],  \n",
    "            hours_info['Sunday']])    \n",
    "            print(\"***************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ca586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV file \n",
    "#\"FindHelp_extracted_data_riv_shelter.csv\" is the name for the shelter csv\n",
    "#\"FindHelp_extracted_data_riv_food_pantry.csv\" is the name for the food pantry csv\n",
    "#\"FindHelp_extracted_data_riv_mental_health.csv\" is the name for the food pantry csv\n",
    "csv_filename = \"FindHelp_extracted_data_riv_mental_health.csv\"\n",
    "\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    # The header \n",
    "    header = [\n",
    "        \"Service_name\",\n",
    "        \"Service_url\",\n",
    "        \"Main_Services\",\n",
    "        \"Other_Services\",\n",
    "        \"Serving\",\n",
    "        \"Phone_Number\",\n",
    "        \"Website\",\n",
    "        \"Location_Address\",\n",
    "        \"Location_URL_Map\",\n",
    "        \"Eligibility\",\n",
    "        \"Availability\",\n",
    "        \"Description\",\n",
    "        \"Languages\",\n",
    "        \"Cost\",\n",
    "        \"Google_Review\",\n",
    "        \"Facebook_URL\",\n",
    "        \"Twitter_URL\",\n",
    "        \"Coverage\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"Zipcode\",\n",
    "        \"24hour\",\n",
    "        \"Monday\",\n",
    "        \"Tuesday\",\n",
    "        \"Wednesday\",\n",
    "        \"Thursday\",\n",
    "        \"Friday\",\n",
    "        \"Saturday\",\n",
    "        \"Sunday\"\n",
    "    ]\n",
    "    csv_writer.writerow(header)\n",
    "    \n",
    "    csv_writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af9adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\"FindHelp_extracted_data_riv_shelter.csv\" is the name for the shelter csv\n",
    "#\"FindHelp_extracted_data_riv_food_pantry.csv\" is the name for the food pantry csv\n",
    "#\"FindHelp_extracted_data_riv_mental_health.csv\" is the name for the food pantry csv\n",
    "df = pd.read_csv(\"FindHelp_extracted_data_riv_mental_health.csv\")\n",
    "\n",
    "riv_zipcodes = [92501, 92502, 92503, 92504, 92505, 92506, 92507, 92508, 92513, 92514, 92516, 92517, 92521, 92522]\n",
    "\n",
    "filtered_df = df[df['Zipcode'].isin(riv_zipcodes) | (df['Location_Address'].isna() & df['Longitude'].isna() & df['Latitude'].isna())]\n",
    "\n",
    "filtered_df = filtered_df.drop_duplicates(subsets = ['Website', 'Service_name'])\n",
    "\n",
    "filtered_df.to_csv(\"FindHelp_extracted_data_riv_mental_health.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb74004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
